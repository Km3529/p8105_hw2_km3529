p8105\_hw2\_km3529
================
Karina Myers

``` r
library(tidyverse)
```

    ## ── Attaching packages ──────────────────────────────────────────────── tidyverse 1.3.0 ──

    ## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
    ## ✓ tibble  3.0.3     ✓ dplyr   1.0.2
    ## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
    ## ✓ readr   1.3.1     ✓ forcats 0.5.0

    ## ── Conflicts ─────────────────────────────────────────────────── tidyverse_conflicts() ──
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(readxl)
```

## Problem 1

``` r
path_to_data = "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx"
```

Read the Mr. Trashwheel dataset

``` r
trashwheel_df = 
    read_xlsx(
        path = path_to_data,
        sheet = "Mr. Trash Wheel",
        range = cell_cols("A:N")) %>% 
    janitor::clean_names() %>% 
    drop_na(dumpster) %>% 
    mutate(
        sports_balls = round(sports_balls),
        sports_balls = as.integer(sports_balls)
    )
```

Read precipitation data for 2017 and 2018

``` r
#setting up 2018
precip_2018 = 
    read_excel(
        "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
        sheet = "2018 Precipitation",
        skip = 1
    ) %>% 
    janitor::clean_names() %>% 
    drop_na(month) %>% 
    mutate(year = 2018) %>% 
    relocate(year)

#setting up 2017
precip_2017 = 
    read_excel(
        "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
        sheet = "2017 Precipitation",
        skip = 1
    ) %>% 
    janitor::clean_names() %>% 
    drop_na(month) %>% 
    mutate(year = 2017) %>% 
    relocate(year)
```

Combine precipitation datasets

``` r
month_df = 
    tibble(
        month = 1:12,
        month_name = month.name
    )
precip_df = 
    bind_rows(precip_2018, precip_2017)
precip_df =
    left_join(precip_df, month_df, by = "month") %>% 
  select(-month) %>% 
  rename(month = month_name) %>%
  relocate(year,month)
```

This dataset contains information from the Mr. Trashwheel trash
collector in Baltimore, Maryland. As trash enters the inner harbor, the
trashwheel collects that trash, and stores it in a dumpster. The dataset
contains information on year, month, and trash collected, include some
specific kinds of trash. There are 344 rows and 14 columns. In this
dataset: The median number of sports balls found in a dumpster in 2017
was 8 Additional data sheets include month and precipitation data. When
looking at precipatation in 2018 and 2017, there are a total of 24 rows
and 3 in our final dataset. The total precipitation in 2018 was 70.33
inches. The total preciptation in 2018 was 32.93 inches.

## Problem 2

Load NYC Transit Data

``` r
transit_df = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  mutate(
      entry = as.character(entry), 
      entry = replace(entry, entry == "YES", "TRUE"), 
        entry = replace(entry, entry == "NO", "FALSE"), 
      entry = as.logical(entry))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   `Station Latitude` = col_double(),
    ##   `Station Longitude` = col_double(),
    ##   Route8 = col_double(),
    ##   Route9 = col_double(),
    ##   Route10 = col_double(),
    ##   Route11 = col_double(),
    ##   ADA = col_logical(),
    ##   `Free Crossover` = col_logical(),
    ##   `Entrance Latitude` = col_double(),
    ##   `Entrance Longitude` = col_double()
    ## )

    ## See spec(...) for full column specifications.

This dataset contains information on the New York City subway system. It
includes data on station lines, station coordinates, entrance types, and
ADA accessibility. It also provides route name information, as many
stops have many lines. After importing the data, the names of variables
were converted to snake case. Certain variables such as ADA notes were
dropped and only the variables of interest were included. Entry was then
converted from a character variable to a logical variable. While the
latitude the NYC subway system range from 40.576127, 40.903125, the
longtidue covers -74.030876, -73.755405. There are a total of 1868 rows
and 19. The data are not tidy because there are multiple columns
describing route names. A tidyier way to present this would be to have a
different row for each route number.

Find the number of distinct stations

``` r
distinct(transit_df, line, station_name) %>% 
  count()
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1   465

ADA compliant stations

``` r
filter(transit_df, ada == TRUE) %>% 
distinct(line, station_name) %>% 
count()
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1    84

Proportion of entrances/exists without vending that allow entrance

``` r
a = filter(transit_df, vending != "YES" 
      & entry == TRUE) %>% 
  count()

b = filter(transit_df, vending != "YES") %>% 
  count()

a/b
```

    ##           n
    ## 1 0.3770492

The proportion of station entrances and exists without vending that
allow entrance is 0.3770492.

Reformat so route number and route name are distinct variables

``` r
transit_tidy =
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = "cccnnccccccccccccccccclclcccnncc") %>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  mutate(
      entry = as.character(entry), 
      entry = replace(entry, entry == "YES", "TRUE"), 
        entry = replace(entry, entry == "NO", "FALSE"), 
      entry = as.logical(entry)) %>% 
  pivot_longer(
    route1:route11, 
    names_to = "route_name", 
    values_to = "route_number") %>% 
  drop_na(route_number)
```

Serve the A Train

``` r
#A Train
filter(transit_tidy, route_number == "A") %>% 
  distinct(line, station_name) %>% 
  count()
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1    60

``` r
# ADA accessible A Train Stations
filter(transit_tidy, 
      route_number == "A" &
        ada == TRUE) %>% 
  distinct(line, station_name) %>% 
  count()
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1    17

## Problem 3

Clean the pols-month data

``` r
month_df = 
  tibble(
      month = 1:12, 
      month_name = month.abb)

pols_df = 
  read_csv("./data/fivethirtyeight_datasets/pols-month.csv") %>% 
  janitor::clean_names() %>% 
  separate(mon, into = c("year", "month", "day")) %>% 
  mutate(month = as.integer(month)) %>% 
  mutate(day = as.integer(day)) %>% 
  mutate(year = as.integer(year))
```

    ## Parsed with column specification:
    ## cols(
    ##   mon = col_date(format = ""),
    ##   prez_gop = col_double(),
    ##   gov_gop = col_double(),
    ##   sen_gop = col_double(),
    ##   rep_gop = col_double(),
    ##   prez_dem = col_double(),
    ##   gov_dem = col_double(),
    ##   sen_dem = col_double(),
    ##   rep_dem = col_double()
    ## )

``` r
pols_df =
  left_join(pols_df, month_df, by = "month")  %>% 
  select(-month, -day) %>% 
  rename(month = month_name) %>% 
  mutate(
      president = case_when(
        prez_dem == 0 ~ "gop", 
        prez_dem == 1 ~ "dem")) %>% 
  relocate(year, month) 
```

Clean the snp data

``` r
 month_df = 
  tibble(
      month = 1:12, 
      month_name = month.abb)

snp_df = 
  read_csv("./data/fivethirtyeight_datasets/snp.csv") %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("month", "day", "year")) %>% 
  mutate(month = as.numeric(month)) %>% 
  mutate(year = as.numeric(year)) %>% 
  mutate(day = as.numeric(day))
```

    ## Parsed with column specification:
    ## cols(
    ##   date = col_character(),
    ##   close = col_double()
    ## )

``` r
snp_df = 
 left_join(snp_df, month_df, by = "month")  %>% 
  select(-month, -day) %>% 
  rename(month = month_name) %>% 
  relocate(year, month)
```

unemployment data

``` r
 month_df = 
  tibble(
      month = 1:12, 
      month_name = month.abb)

unemployment_df = 
  read_csv("./data/fivethirtyeight_datasets/unemployment.csv") %>% 
  janitor::clean_names()
```

    ## Parsed with column specification:
    ## cols(
    ##   Year = col_double(),
    ##   Jan = col_double(),
    ##   Feb = col_double(),
    ##   Mar = col_double(),
    ##   Apr = col_double(),
    ##   May = col_double(),
    ##   Jun = col_double(),
    ##   Jul = col_double(),
    ##   Aug = col_double(),
    ##   Sep = col_double(),
    ##   Oct = col_double(),
    ##   Nov = col_double(),
    ##   Dec = col_double()
    ## )

``` r
unemployment_tidy =
  unemployment_df %>% 
  pivot_longer(
    jan:dec,
    names_to = "month", 
    values_to = "unemp") %>% 
  mutate(month = str_to_title(month))
```

``` r
merge_df = 
  left_join(pols_df, snp_df, by = c("year", "month"))
  
final_df = 
  left_join(merge_df, unemployment_tidy, by = c("year", "month"))
```

The pols-month dataset contains political party composition per month
between the years of 1947, 2015. It contains political party information
on senators, governors, representatives, and the president. The snp
dataset contains information about the Standard & Poor’s stock market
index (S\&P). This index is used as a measure of the stock market as a
whole. S\&P values were collected every month between 1950, 2015. The
recorded scores range between 17.049999, 2107.389893 and the average
closing score was 474.8887404. The unemployment dataset contains the
percent of people unemployed in the United States between the years of
1948, 2015. During this time, the median unemployment rate was NA.

After merging the three dataset, there are 822 rows and 13. It contains
information on political partly affiliations of presidents, senators,
representatives and governors, the unemployment rate, and the S\&P
closing index between the years of 1947, 2015. During the time, average
unemployment rate was NA and it ranged between \`r
range(final\_df$unemp).
